{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMxMIl_Iwm9D",
        "outputId": "99b6f51f-9ce4-4a39-864a-0158737951fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#pandas installation\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries for this task\n",
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "no7If-gvwoiX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive to access files stored there\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYIOwGF-_LKt",
        "outputId": "29dcdeef-b060-4a08-aca1-623a9da026d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_convert_to_csv(json_file_path, output_folder):\n",
        "   \"\"\"\n",
        "    Reads a line-delimited JSON file, extracts up to 5000 valid entries,\n",
        "    and saves them as a CSV file in the specified output folder.\n",
        "    This is done for each JSON file.\n",
        "    This proceess was for a total of 7 JSON files containing different categories of online shopping review entries.\n",
        "    Each of these 7 JSON files contained 50,00,000 entries and 5,000 valid entries from each of these have been extracted and saved as individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        json_file_path (str): Full path to the .json file (line-delimited format).\n",
        "        output_folder (str): Folder path where you want the resulting CSV to be saved.\n",
        "\n",
        "    Returns:\n",
        "        None. A CSV file named 'separate.csv' is saved in the output folder.\n",
        "    \"\"\"\n",
        "    # This list will hold all the valid JSON entries we want to convert\n",
        "    valid_rows = []\n",
        "\n",
        "    # Open the file and read each line one by one\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        # Read the JSON lines in the file\n",
        "        for line in f:\n",
        "            try:\n",
        "                # Load each line as a JSON object\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # If it's a dictionary (i.e., a valid review or record), we keep it\n",
        "                if isinstance(data, dict):\n",
        "                    valid_rows.append(data)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "              # If the line is not valid JSON, skip it and move on\n",
        "                continue\n",
        "\n",
        "            # Stop once we've collected 5000 good entries\n",
        "            if len(valid_rows) >= 5000:\n",
        "                break\n",
        "\n",
        "    # Now convert all our collected JSON dicts into a Pandas DataFrame\n",
        "    df = pd.DataFrame(valid_rows)\n",
        "\n",
        "    # Define where we want to save the CSV file\n",
        "    output_csv_path = f'{output_folder}/separate.csv'\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"CSV file saved at: {output_csv_path}\")\n",
        "\n",
        "# Input the path to the JSON file where we load the .json file and the output folder where we save our .csv file\n",
        "json_file_path = '/content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Amazon Reviews Dataset/separate.json/separate.json'\n",
        "output_folder = '/content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets'\n",
        "\n",
        "#Calling the function for conversion\n",
        "extract_and_convert_to_csv(json_file_path, output_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fikRIN8-xMbq",
        "outputId": "df02487d-5b00-4043-a180-d99ca966ce2b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved at: /content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets/separate.csv\n"
          ]
        }
      ]
    }
  ]
}