{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCjvojgCAU5R",
        "outputId": "770e3276-0b6f-458d-e7bd-3e627cc63326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "a3vbXjMnAXtN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CToItD1yAh6s",
        "outputId": "38487e0d-b376-491e-d5f9-d76c7082c00c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def three_way_split_and_save(merged_csv_path, output_folder, stratify_column=None):\n",
        "    # Loading the merged dataset\n",
        "     \"\"\"\n",
        "    Splits a merged dataset into train, validation, and test sets\n",
        "    (70%, 15%, 15%) and saves them as separate CSV files.\n",
        "\n",
        "    Optionally supports stratified splitting based on a specified column\n",
        "    to preserve class distribution across splits.\n",
        "\n",
        "    Args:\n",
        "        merged_csv_path (str): Full path to the merged CSV dataset.\n",
        "        output_folder (str): Directory where the split CSV files should be saved.\n",
        "        stratify_column (str, optional): Column name to stratify on, if desired.\n",
        "                                         Defaults to None (no stratification).\n",
        "\n",
        "    Returns:\n",
        "        None. Saves 'train_dataset.csv', 'val_dataset.csv', and 'test_dataset.csv'\n",
        "        in the output folder.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(merged_csv_path)\n",
        "\n",
        "    # Set stratification values if a valid stratify column is provided\n",
        "    stratify_vals = df[stratify_column] if stratify_column and stratify_column in df.columns else None\n",
        "\n",
        "    # First, split 70% for training and 30% as a temporary set\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=stratify_vals)\n",
        "\n",
        "    # Then split the temporary set equally into validation and test sets (15% each)\n",
        "    stratify_temp = temp_df[stratify_column] if stratify_column and stratify_column in df.columns else None\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=stratify_temp)\n",
        "\n",
        "    # Define the paths for each of the split datasets\n",
        "    train_path = os.path.join(output_folder, 'train_dataset.csv')\n",
        "    val_path = os.path.join(output_folder, 'val_dataset.csv')\n",
        "    test_path = os.path.join(output_folder, 'test_dataset.csv')\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    # Save each split to CSV (no index column)\n",
        "    print(f\"Training set saved: {train_path} — {len(train_df)} rows\")\n",
        "    print(f\"Validation set saved: {val_path} — {len(val_df)} rows\")\n",
        "    print(f\"Test set saved: {test_path} — {len(test_df)} rows\")\n",
        "\n",
        "# Input your paths\n",
        "merged_csv_path = '/content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets/merged_output.csv'\n",
        "output_folder = '/content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets'\n",
        "\n",
        "# Run the function (no stratification for now)\n",
        "three_way_split_and_save(merged_csv_path, output_folder, stratify_column=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBEbbH3XAdpr",
        "outputId": "49a413c9-a9ab-41ff-d64e-57cb3b927855"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set saved: /content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets/train_dataset.csv — 24500 rows\n",
            "Validation set saved: /content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets/val_dataset.csv — 5250 rows\n",
            "Test set saved: /content/drive/MyDrive/GW Semesters/Sem 2/ML by Prof Shi Feng/ML Sem Project - Fake Review Detector/FakeReviewShield/Final Datasets/test_dataset.csv — 5250 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBhHNoGrAgVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}